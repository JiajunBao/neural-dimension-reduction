{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sx9e_pXlCuti"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMMut8UVCutt"
   },
   "source": [
    "# Experiments\n",
    "We'll go through learning feature embeddings using different loss functions on MNIST dataset. This is just for visualization purposes, thus we'll be using 2-dimensional embeddings which isn't the best choice in practice.\n",
    "\n",
    "For every experiment the same embedding network is used (32 conv 5x5 -> PReLU -> MaxPool 2x2 -> 64 conv 5x5 -> PReLU -> MaxPool 2x2 -> Dense 256 -> PReLU -> Dense 256 -> PReLU -> Dense 2) and we don't do any hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcmGBqXeCutw"
   },
   "source": [
    "# Prepare dataset\n",
    "We'll be working on MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcZTFRnjCut3"
   },
   "source": [
    "## Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Dz2xh66UCut5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.models.siamese_triplet.trainer import fit\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "          '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "          '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, ncluster, xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    inds = np.where(targets==ncluster)[0]\n",
    "    plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[ncluster])  # np.random.rand(3,)\n",
    "    for i in range(ncluster):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])  # np.random.rand(3,)\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(mnist_classes)\n",
    "\n",
    "\n",
    "def get_labels(dataloader, label_anchors):\n",
    "    if label_anchors.shape[0] == 1:\n",
    "        x1 = dataloader.dataset.data[label_anchors].unsqueeze(dim=0)\n",
    "    else:\n",
    "        x1 = dataloader.dataset.data[label_anchors]\n",
    "    dist = torch.cdist(x1=x1, x2=dataloader.dataset.data, p=2)  # (n, n)\n",
    "    sorted_dist, indices = torch.sort(dist, dim=1, descending=False)\n",
    "    \n",
    "    num_anchors = label_anchors.shape[0]\n",
    "    n = dataloader.dataset.data.shape[0]\n",
    "    labels = - torch.ones(n)\n",
    "    for i in range(num_anchors):\n",
    "        labels[indices[i, :100]] = i\n",
    "    labels[labels==-1.] = num_anchors\n",
    "    return labels\n",
    "    \n",
    "\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        size = dataloader.dataset.data.shape[0]\n",
    "        batch_size = 256\n",
    "        embedded_x_list = list()\n",
    "        for i in tqdm(range(size // batch_size + 1)):\n",
    "            batch_x = dataloader.dataset.data[batch_size * i: batch_size * (i + 1)].float().to(device)\n",
    "            embedded_x = model.get_embedding(batch_x).to('cpu')\n",
    "            embedded_x_list.append(embedded_x)\n",
    "    return torch.cat(embedded_x_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jv4DvFucCuuu"
   },
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "from src.models.siamese_triplet.datasets import TripletSynthesis\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "triplet_train_dataset = TripletSynthesis(Path('../data/train.pt')) # Returns triplets of images\n",
    "triplet_test_dataset = TripletSynthesis(Path('../data/train.pt'))\n",
    "batch_size = 2048\n",
    "kwargs = {'num_workers': 32, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from src.models.siamese_triplet.networks import EmbeddingNet, TripletNet\n",
    "from src.models.siamese_triplet.losses import TripletLoss\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "model = model.to(device)\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 1\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "Dj9AoYpsCuuz",
    "outputId": "70ff7e3d-4e0b-403c-9af1-41c51a4c808e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model, lowest_val_loss = fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "ysh4Ry7ZCuu_",
    "outputId": "4194cf1d-da83-452a-94d6-2cd4c31aca2a"
   },
   "outputs": [],
   "source": [
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_tl, train_labels_tl)\n",
    "val_embeddings_tl, val_labels_tl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_tl, val_labels_tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7C9H1_nCuvJ"
   },
   "source": [
    "# Online pair/triplet selection - negative mining\n",
    "There are couple of problems with siamese and triplet networks.\n",
    "1. The number of possible pairs/triplets grows **quadratically/cubically** with the number of examples. It's infeasible to process them all\n",
    "2. We generate pairs/triplets randomly. As the training continues, more and more pairs/triplets are easy to deal with (their loss value is very small or even 0), preventing the network from training. We need to provide the network with **hard examples**.\n",
    "3. Each image that is fed to the network is used only for computation of contrastive/triplet loss for only one pair/triplet. The computation is somewhat wasted; once the embedding is computed, it could be reused for many pairs/triplets.\n",
    "\n",
    "To deal with that efficiently, we'll feed a network with standard mini-batches as we did for classification. The loss function will be responsible for selection of hard pairs and triplets within mini-batch. In these case, if we feed the network with 16 images per 10 classes, we can process up to $159*160/2 = 12720$ pairs and $10*16*15/2*(9*16) = 172800$ triplets, compared to 80 pairs and 53 triplets in previous implementation.\n",
    "\n",
    "We can find some strategies on how to select triplets in [2] and [3] *Alexander Hermans, Lucas Beyer, Bastian Leibe, [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/pdf/1703.07737), 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k806qej9CuvL"
   },
   "source": [
    "## Online pair selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **PairSelector** that takes embeddings and original labels and returns valid pairs within a minibatch\n",
    "5. Define **OnlineContrastiveLoss** that will use a *PairSelector* and compute *ContrastiveLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "goVi1_-PCuvL"
   },
   "outputs": [],
   "source": [
    "from src.models.siamese_triplet.datasets import BalancedBatchSampler\n",
    "\n",
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from src.models.siamese_triplet.networks import EmbeddingNet\n",
    "from src.models.siamese_triplet.losses import OnlineContrastiveLoss\n",
    "from src.models.siamese_triplet.utils import AllPositivePairSelector, HardNegativePairSelector # Strategies for selecting pairs within a minibatch\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineContrastiveLoss(margin, HardNegativePairSelector())\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "SacMTqN6CuvO",
    "outputId": "d339ea0e-a143-423f-939a-55f0b9a38e6a"
   },
   "outputs": [],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "ZCvF0AlCCuvX",
    "outputId": "84044105-6289-4a9f-9ec5-47a7d539651a"
   },
   "outputs": [],
   "source": [
    "train_embeddings_ocl, train_labels_ocl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_ocl, train_labels_ocl)\n",
    "val_embeddings_ocl, val_labels_ocl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_ocl, val_labels_ocl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgIGiMwICuvn"
   },
   "source": [
    "## Online triplet selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **TripletSelector** that takes embeddings and original labels and returns valid triplets within a minibatch\n",
    "5. Define **OnlineTripletLoss** that will use a *TripletSelector* and compute *TripletLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JzpYzMUuCuvp"
   },
   "outputs": [],
   "source": [
    "from src.models.siamese_triplet.datasets import BalancedBatchSampler\n",
    "\n",
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from src.models.siamese_triplet.networks import EmbeddingNet\n",
    "from src.models.siamese_triplet.losses import OnlineTripletLoss\n",
    "from src.models.siamese_triplet.utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
    "from src.models.siamese_triplet.metrics import AverageNonzeroTripletsMetric\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "W-bDxqVJCuvs",
    "outputId": "0b3f7143-d6e5-4f74-9e70-17405026cc91"
   },
   "outputs": [],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "dNrbA2hCCuvw",
    "outputId": "df17047b-8a35-4370-ebe7-be83e9154003"
   },
   "outputs": [],
   "source": [
    "train_embeddings_otl, train_labels_otl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_otl, train_labels_otl)\n",
    "val_embeddings_otl, val_labels_otl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_otl, val_labels_otl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "tNY7NReM0zsq",
    "outputId": "5c4db690-d4db-4182-c5d6-9c8d6fc816e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display_emb_online, display_emb, display_label_online, display_label = train_embeddings_otl, train_embeddings_tl, train_labels_otl, train_labels_tl\n",
    "display_emb_online, display_emb, display_label_online, display_label = val_embeddings_otl, val_embeddings_tl, val_labels_otl, val_labels_tl\n",
    "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
    "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
    "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
    "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "-vyd_460yt7n",
    "outputId": "68c58b88-059f-4b04-ddee-3d3a59bc1856"
   },
   "outputs": [],
   "source": [
    "x_lim = (np.min(train_embeddings_ocl[:,0]), np.max(train_embeddings_ocl[:,0]))\n",
    "y_lim = (np.min(train_embeddings_ocl[:,1]), np.max(train_embeddings_ocl[:,1]))\n",
    "plot_embeddings(train_embeddings_cl, train_labels_cl, x_lim, y_lim)\n",
    "plot_embeddings(train_embeddings_ocl, train_labels_ocl, x_lim, y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "smvUHdXOyt7r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Experiments_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
