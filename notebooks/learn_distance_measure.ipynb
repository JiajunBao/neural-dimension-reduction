{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jiajunb/neural-dimension-reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f96a00d5830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.models.distance_modeling import SurveyorDataSet, Surveyor, thesis_kl_div_add_mse_loss\n",
    "\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/processed/sample/train.csv')\n",
    "val_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/processed/sample/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-5\n",
    "num_epoches = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = Surveyor()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, verbose):\n",
    "    model.train()\n",
    "    loss_sum = 0.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x1, x2, labels, q = batch\n",
    "        x1, x2, labels, q = x1.to(device), x2.to(device), labels.to(device), q.to(device)\n",
    "        logits, p, out1, out2, loss = model(x1, x2, q, labels)\n",
    "        model.zero_grad()  # reset gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        if verbose and i % 20 == 0:\n",
    "            print(f'training loss: {loss_sum / (i + 1):.4f}')\n",
    "    return loss_sum / len(train_loader)\n",
    "\n",
    "def val_one_epoch(val_loader, model):\n",
    "    model.eval()\n",
    "    loss_fn1 = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = thesis_kl_div_add_mse_loss\n",
    "    preds_list = list()\n",
    "    labels_list = list()\n",
    "    val_xentropy_loss = 0.\n",
    "    val_thesis_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x1, x2, labels, q = batch\n",
    "            x1, x2, q = x1.to(device), x2.to(device), q.to(device)\n",
    "            logits, p, out1, out2 = model(x1, x2, q, labels=None)\n",
    "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            preds_list.append(preds.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            labels = labels.to(device)\n",
    "            val_xentropy_loss += loss_fn1(logits, labels).item()\n",
    "            val_thesis_loss += loss_fn2(p, q).item()\n",
    "    y_preds = torch.cat(preds_list)\n",
    "    y_golds = torch.cat(labels_list)\n",
    "    accuracy = float((y_preds == y_golds).sum().item()) / len(y_preds)\n",
    "    return val_xentropy_loss / len(y_preds), val_thesis_loss / len(y_preds), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, verbose):\n",
    "    best_model = None\n",
    "    best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = float('inf'), float('inf'), 0. \n",
    "    for epoch_idx in range(1, num_epoches + 1):\n",
    "        avg_loss = train_one_epoch(train_loader, model, optimizer, False)\n",
    "        avg_xentropy_loss, avg_thesis_loss, val_accuracy = val_one_epoch(val_loader, model)\n",
    "        if val_accuracy >  best_val_accuracy:\n",
    "            best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = avg_xentropy_loss, avg_thesis_loss, val_accuracy\n",
    "            best_model = copy.deepcopy(model.cpu())\n",
    "        if verbose and (epoch_idx) % 40 == 0:\n",
    "            print(f'epoch [{epoch_idx}]/[{num_epoches}] training loss: {avg_loss:.4f} '\n",
    "                  f'val_cross_entropy_loss: {avg_xentropy_loss:.4f} '\n",
    "                  f'val_thesis_loss: {avg_thesis_loss:.4f} '\n",
    "                  f'val_accuracy: {val_accuracy:.4f} ')\n",
    "    return best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [40]/[1200] training loss: 63.3532 val_cross_entropy_loss: 0.0010 val_thesis_loss: 0.1186 val_accuracy: 0.6239 \n",
      "epoch [80]/[1200] training loss: 41.7057 val_cross_entropy_loss: 0.0010 val_thesis_loss: 0.1108 val_accuracy: 0.6554 \n",
      "epoch [120]/[1200] training loss: 29.2389 val_cross_entropy_loss: 0.0010 val_thesis_loss: 0.1070 val_accuracy: 0.6532 \n",
      "epoch [160]/[1200] training loss: 20.0840 val_cross_entropy_loss: 0.0010 val_thesis_loss: 0.1055 val_accuracy: 0.6779 \n",
      "epoch [200]/[1200] training loss: 13.1818 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.1056 val_accuracy: 0.6862 \n",
      "epoch [240]/[1200] training loss: 7.7870 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0979 val_accuracy: 0.7155 \n",
      "epoch [280]/[1200] training loss: 1.2100 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0916 val_accuracy: 0.7110 \n",
      "epoch [320]/[1200] training loss: -8.2647 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0853 val_accuracy: 0.7080 \n",
      "epoch [360]/[1200] training loss: -11.0524 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0819 val_accuracy: 0.7065 \n",
      "epoch [400]/[1200] training loss: -15.3719 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0759 val_accuracy: 0.7365 \n",
      "epoch [440]/[1200] training loss: -19.2413 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0734 val_accuracy: 0.7545 \n",
      "epoch [480]/[1200] training loss: -20.6145 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0703 val_accuracy: 0.7680 \n",
      "epoch [520]/[1200] training loss: -24.5661 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0687 val_accuracy: 0.7980 \n",
      "epoch [560]/[1200] training loss: -26.5977 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0681 val_accuracy: 0.8386 \n",
      "epoch [600]/[1200] training loss: -28.3625 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0659 val_accuracy: 0.8619 \n",
      "epoch [640]/[1200] training loss: -30.0625 val_cross_entropy_loss: 0.0009 val_thesis_loss: 0.0648 val_accuracy: 0.8664 \n",
      "epoch [680]/[1200] training loss: -33.4110 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0628 val_accuracy: 0.8754 \n",
      "epoch [720]/[1200] training loss: -34.2871 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0604 val_accuracy: 0.8716 \n",
      "epoch [760]/[1200] training loss: -37.1378 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0582 val_accuracy: 0.8746 \n",
      "epoch [800]/[1200] training loss: -38.5067 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0573 val_accuracy: 0.8836 \n",
      "epoch [840]/[1200] training loss: -39.8574 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0580 val_accuracy: 0.8911 \n",
      "epoch [880]/[1200] training loss: -41.9725 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0544 val_accuracy: 0.8889 \n",
      "epoch [920]/[1200] training loss: -43.2749 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0528 val_accuracy: 0.8866 \n",
      "epoch [960]/[1200] training loss: -42.9565 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0531 val_accuracy: 0.8851 \n",
      "epoch [1000]/[1200] training loss: -45.1111 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0527 val_accuracy: 0.8911 \n",
      "epoch [1040]/[1200] training loss: -45.3468 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0507 val_accuracy: 0.9032 \n",
      "epoch [1080]/[1200] training loss: -48.4130 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0503 val_accuracy: 0.9002 \n",
      "epoch [1120]/[1200] training loss: -48.0956 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0513 val_accuracy: 0.9167 \n",
      "epoch [1160]/[1200] training loss: -50.6104 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0514 val_accuracy: 0.9212 \n",
      "epoch [1200]/[1200] training loss: -50.1460 val_cross_entropy_loss: 0.0008 val_thesis_loss: 0.0500 val_accuracy: 0.9182 \n"
     ]
    }
   ],
   "source": [
    "best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, final_model = train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"best_model\": best_model.state_dict(),\n",
    "    \"best_avg_xentropy_loss\": best_avg_xentropy_loss,\n",
    "    \"best_avg_thesis_loss\": best_avg_thesis_loss, \n",
    "    \"best_val_accuracy\": best_val_accuracy\n",
    "}, '../saves/surveyor.on.sample.0.92.12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
