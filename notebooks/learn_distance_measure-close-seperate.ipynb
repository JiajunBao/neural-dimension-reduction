{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jiajunb/neural-dimension-reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efe04068870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.models.distance_modeling import SurveyorDataSet, Surveyor, thesis_kl_div_add_mse_loss\n",
    "\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe7ccc3177349ab90743516d9b090f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='create triplets', max=4.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d929ef54790f4f6693a4b4013598e5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='create triplets', max=2.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def far_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, 2].reshape(-1, 1), indices[:, 2].reshape(-1, 1)\n",
    "\n",
    "def close_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, 1].reshape(-1, 1), indices[:, 1].reshape(-1, 1)\n",
    "\n",
    "train_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/processed/sample/train.csv', close_func2, far_func2)\n",
    "val_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/processed/sample/dev.csv', close_func2, far_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "model = Surveyor()\n",
    "\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, verbose):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_sum = 0.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x1, x2, labels, q = batch\n",
    "        x1, x2, labels, q = x1.to(device), x2.to(device), labels.to(device), q.to(device)\n",
    "        logits, p, out1, out2, loss = model(x1, x2, q, labels)\n",
    "        model.zero_grad()  # reset gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        if verbose and i % 20 == 0:\n",
    "            print(f'training loss: {loss_sum / (i + 1):.4f}')\n",
    "    return loss_sum / len(train_loader)\n",
    "\n",
    "def val_one_epoch(val_loader, model):\n",
    "    model.eval()\n",
    "    loss_fn1 = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = thesis_kl_div_add_mse_loss\n",
    "    preds_list = list()\n",
    "    labels_list = list()\n",
    "    val_xentropy_loss = 0.\n",
    "    val_thesis_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x1, x2, labels, q = batch\n",
    "            x1, x2, q = x1.to(device), x2.to(device), q.to(device)\n",
    "            logits, p, out1, out2 = model(x1, x2, q, labels=None)\n",
    "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            preds_list.append(preds.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            labels = labels.to(device)\n",
    "            val_xentropy_loss += loss_fn1(logits, labels).item()\n",
    "            val_thesis_loss += loss_fn2(p, q).item()\n",
    "    y_preds = torch.cat(preds_list)\n",
    "    y_golds = torch.cat(labels_list)\n",
    "    accuracy = float((y_preds == y_golds).sum().item()) / len(y_preds)\n",
    "    return val_xentropy_loss / len(y_preds), val_thesis_loss / len(y_preds), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, verbose):\n",
    "    best_model = None\n",
    "    best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = float('inf'), float('inf'), 0. \n",
    "    for epoch_idx in range(1, num_epoches + 1):\n",
    "        avg_loss = train_one_epoch(train_loader, model, optimizer, False)\n",
    "        avg_xentropy_loss, avg_thesis_loss, val_accuracy = val_one_epoch(val_loader, model)\n",
    "        if val_accuracy >  best_val_accuracy:\n",
    "            best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = avg_xentropy_loss, avg_thesis_loss, val_accuracy\n",
    "            best_model = copy.deepcopy(model.cpu())\n",
    "        if verbose and (epoch_idx) % 5 == 0:\n",
    "            print(f'epoch [{epoch_idx}]/[{num_epoches}] training loss: {avg_loss:.4f} '\n",
    "                  f'val_cross_entropy_loss: {avg_xentropy_loss:.4f} '\n",
    "                  f'val_thesis_loss: {avg_thesis_loss:.4f} '\n",
    "                  f'val_accuracy: {val_accuracy:.4f} ')\n",
    "    return best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [5]/[400] training loss: -73.5262 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.0228 val_accuracy: 0.4947 \n",
      "epoch [10]/[400] training loss: -107.3055 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1035 val_accuracy: 0.5038 \n",
      "epoch [15]/[400] training loss: -118.6596 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.0968 val_accuracy: 0.5030 \n",
      "epoch [20]/[400] training loss: -124.5892 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1063 val_accuracy: 0.5030 \n",
      "epoch [25]/[400] training loss: -128.9137 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1087 val_accuracy: 0.5015 \n",
      "epoch [30]/[400] training loss: -133.1475 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1089 val_accuracy: 0.4992 \n",
      "epoch [35]/[400] training loss: -141.3520 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1133 val_accuracy: 0.5053 \n",
      "epoch [40]/[400] training loss: -143.4393 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1155 val_accuracy: 0.5038 \n",
      "epoch [45]/[400] training loss: -145.8129 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1159 val_accuracy: 0.5098 \n",
      "epoch [50]/[400] training loss: -146.6887 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1177 val_accuracy: 0.4917 \n",
      "epoch [55]/[400] training loss: -148.6273 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1184 val_accuracy: 0.5083 \n",
      "epoch [60]/[400] training loss: -149.8384 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1185 val_accuracy: 0.5023 \n",
      "epoch [65]/[400] training loss: -150.8986 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1185 val_accuracy: 0.5030 \n",
      "epoch [70]/[400] training loss: -150.9945 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1192 val_accuracy: 0.4985 \n",
      "epoch [75]/[400] training loss: -151.6930 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1189 val_accuracy: 0.5045 \n",
      "epoch [80]/[400] training loss: -151.3032 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1200 val_accuracy: 0.4940 \n",
      "epoch [85]/[400] training loss: -152.0427 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1200 val_accuracy: 0.4947 \n",
      "epoch [90]/[400] training loss: -152.3118 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1207 val_accuracy: 0.4850 \n",
      "epoch [95]/[400] training loss: -152.9386 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1199 val_accuracy: 0.5015 \n",
      "epoch [100]/[400] training loss: -152.6713 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1197 val_accuracy: 0.5098 \n",
      "epoch [105]/[400] training loss: -153.2174 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1207 val_accuracy: 0.4962 \n",
      "epoch [110]/[400] training loss: -153.5343 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1208 val_accuracy: 0.4962 \n",
      "epoch [115]/[400] training loss: -153.7114 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1211 val_accuracy: 0.5030 \n",
      "epoch [120]/[400] training loss: -153.8962 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1213 val_accuracy: 0.4805 \n",
      "epoch [125]/[400] training loss: -154.3069 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1216 val_accuracy: 0.5105 \n",
      "epoch [130]/[400] training loss: -154.2587 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1214 val_accuracy: 0.4925 \n",
      "epoch [135]/[400] training loss: -154.5076 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1216 val_accuracy: 0.5038 \n",
      "epoch [140]/[400] training loss: -154.2592 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1221 val_accuracy: 0.5008 \n",
      "epoch [145]/[400] training loss: -154.2374 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1228 val_accuracy: 0.5075 \n",
      "epoch [150]/[400] training loss: -154.4526 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1222 val_accuracy: 0.4895 \n",
      "epoch [155]/[400] training loss: -154.6258 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1224 val_accuracy: 0.5008 \n",
      "epoch [160]/[400] training loss: -154.6777 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1227 val_accuracy: 0.5015 \n",
      "epoch [165]/[400] training loss: -154.6463 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1226 val_accuracy: 0.4895 \n",
      "epoch [170]/[400] training loss: -154.8219 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1229 val_accuracy: 0.5030 \n",
      "epoch [175]/[400] training loss: -154.7505 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1231 val_accuracy: 0.4850 \n",
      "epoch [180]/[400] training loss: -154.7879 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1229 val_accuracy: 0.4737 \n",
      "epoch [185]/[400] training loss: -154.9959 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1235 val_accuracy: 0.5053 \n",
      "epoch [190]/[400] training loss: -154.9098 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1234 val_accuracy: 0.4820 \n",
      "epoch [195]/[400] training loss: -155.0539 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1239 val_accuracy: 0.5038 \n",
      "epoch [200]/[400] training loss: -155.1596 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1238 val_accuracy: 0.4827 \n",
      "epoch [205]/[400] training loss: -155.0830 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1238 val_accuracy: 0.4955 \n",
      "epoch [210]/[400] training loss: -154.9114 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1240 val_accuracy: 0.4910 \n",
      "epoch [215]/[400] training loss: -154.8904 val_cross_entropy_loss: 0.0010 val_thesis_loss: -0.1241 val_accuracy: 0.5105 \n",
      "epoch [220]/[400] training loss: -155.1251 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1241 val_accuracy: 0.5030 \n",
      "epoch [225]/[400] training loss: -155.2140 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1240 val_accuracy: 0.4895 \n",
      "epoch [230]/[400] training loss: -155.0005 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1244 val_accuracy: 0.4962 \n",
      "epoch [235]/[400] training loss: -155.1628 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1244 val_accuracy: 0.4872 \n",
      "epoch [240]/[400] training loss: -155.0125 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1245 val_accuracy: 0.5128 \n",
      "epoch [245]/[400] training loss: -155.0785 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1245 val_accuracy: 0.4947 \n",
      "epoch [250]/[400] training loss: -154.9461 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1244 val_accuracy: 0.5000 \n",
      "epoch [255]/[400] training loss: -155.1366 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1245 val_accuracy: 0.5105 \n",
      "epoch [260]/[400] training loss: -155.2497 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1248 val_accuracy: 0.4887 \n",
      "epoch [265]/[400] training loss: -155.0292 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1246 val_accuracy: 0.5143 \n",
      "epoch [270]/[400] training loss: -155.3338 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1246 val_accuracy: 0.4895 \n",
      "epoch [275]/[400] training loss: -155.4211 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1249 val_accuracy: 0.5045 \n",
      "epoch [280]/[400] training loss: -155.3135 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1249 val_accuracy: 0.5083 \n",
      "epoch [285]/[400] training loss: -155.5001 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1248 val_accuracy: 0.4977 \n",
      "epoch [290]/[400] training loss: -155.4425 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1250 val_accuracy: 0.4887 \n",
      "epoch [295]/[400] training loss: -155.4597 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1249 val_accuracy: 0.5023 \n",
      "epoch [300]/[400] training loss: -155.4933 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1249 val_accuracy: 0.5075 \n",
      "epoch [305]/[400] training loss: -155.5415 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1250 val_accuracy: 0.5068 \n",
      "epoch [310]/[400] training loss: -155.5542 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1250 val_accuracy: 0.4947 \n",
      "epoch [315]/[400] training loss: -155.5242 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1251 val_accuracy: 0.5083 \n",
      "epoch [320]/[400] training loss: -155.4649 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1253 val_accuracy: 0.5030 \n",
      "epoch [325]/[400] training loss: -155.4513 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1253 val_accuracy: 0.5053 \n",
      "epoch [330]/[400] training loss: -155.4713 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1253 val_accuracy: 0.5105 \n",
      "epoch [335]/[400] training loss: -155.4364 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1251 val_accuracy: 0.4985 \n",
      "epoch [340]/[400] training loss: -155.5329 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1252 val_accuracy: 0.5015 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [345]/[400] training loss: -155.4276 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1251 val_accuracy: 0.5098 \n",
      "epoch [350]/[400] training loss: -155.6027 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1254 val_accuracy: 0.5120 \n",
      "epoch [355]/[400] training loss: -155.5856 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1253 val_accuracy: 0.5045 \n",
      "epoch [360]/[400] training loss: -155.5714 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1254 val_accuracy: 0.4932 \n",
      "epoch [365]/[400] training loss: -155.4609 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1254 val_accuracy: 0.5000 \n",
      "epoch [370]/[400] training loss: -155.5306 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1256 val_accuracy: 0.5195 \n",
      "epoch [375]/[400] training loss: -155.5982 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1257 val_accuracy: 0.5015 \n",
      "epoch [380]/[400] training loss: -155.4697 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1256 val_accuracy: 0.4887 \n",
      "epoch [385]/[400] training loss: -155.5335 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1255 val_accuracy: 0.4947 \n",
      "epoch [390]/[400] training loss: -155.4145 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1255 val_accuracy: 0.4932 \n",
      "epoch [395]/[400] training loss: -155.4817 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1256 val_accuracy: 0.5143 \n",
      "epoch [400]/[400] training loss: -155.4599 val_cross_entropy_loss: 0.0011 val_thesis_loss: -0.1257 val_accuracy: 0.5053 \n"
     ]
    }
   ],
   "source": [
    "best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, final_model = train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010902509764508085, -0.12539889075018623, 0.521021021021021)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     \"best_model\": best_model.state_dict(),\n",
    "#     \"best_avg_xentropy_loss\": best_avg_xentropy_loss,\n",
    "#     \"best_avg_thesis_loss\": best_avg_thesis_loss, \n",
    "#     \"best_val_accuracy\": best_val_accuracy\n",
    "# }, '../saves/surveyor.on.full.100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
