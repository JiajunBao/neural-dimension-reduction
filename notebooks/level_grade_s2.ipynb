{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.level_kv_div import binaryTrainer, utils, network\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 points 404000 data\n",
      "mutual negihbors: 147722, one-direction neighbor: 54278, not neighbor: 202000\n",
      "666 points 134532 data\n",
      "mutual negihbors: 51636, one-direction neighbor: 15630, not neighbor: 67266\n"
     ]
    }
   ],
   "source": [
    "train_dataset = binaryTrainer.get_dataset('../data/processed/sample/train.csv', '../data/processed/sample/train.level.grading')\n",
    "dev_dataset = binaryTrainer.get_dataset('../data/processed/sample/dev.csv', '../data/processed/sample/dev.level.grading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network.SiameseNet(network.EmbeddingNet())\n",
    "learning_rate = 2e-3\n",
    "batch_size = 2000\n",
    "num_epoches = 100\n",
    "\n",
    "verbose = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "weight_decay = 1e-6\n",
    "log_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=batch_size, pin_memory=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [3]/[100] training loss: 0.0002 avg_val_margin_loss: 0.0021 train_accuracy:  0.76 val_accuracy:  0.83 \n",
      "epoch [6]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0017 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [9]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0017 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [12]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0016 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [15]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0014 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [18]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0014 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [21]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0011 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [24]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0011 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [27]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [30]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0011 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [33]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0011 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [36]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [39]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0011 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [42]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [45]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [48]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [51]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.80 val_accuracy:  0.83 \n",
      "epoch [54]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [57]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [60]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [63]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0014 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [66]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [69]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [72]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [75]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [78]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [81]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [84]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0014 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [87]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0012 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [90]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [93]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n",
      "epoch [96]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.78 val_accuracy:  0.83 \n",
      "epoch [99]/[100] training loss: 0.0001 avg_val_margin_loss: 0.0013 train_accuracy:  0.79 val_accuracy:  0.83 \n"
     ]
    }
   ],
   "source": [
    "best_avg_val_kl_loss, best_model, model = binaryTrainer.train_with_eval(train_loader, dev_loader, model, optimizer, num_epoches, log_epoch, verbose, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_avg_val_kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = binaryTrainer.TriMarginLoss(1, 3, 4, 6, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 18.02503204345703\n"
     ]
    }
   ],
   "source": [
    "output1, output2 = model(train_dataset[0][0].unsqueeze(0).to(device), train_dataset[0][1].unsqueeze(0).to(device))\n",
    "loss, dist = criterion.forward(output1, output2, train_dataset[0][2])\n",
    "print(loss.item(), dist.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.0250, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((output1 - output2) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = binaryTrainer.get_dataset('../data/train.csv', '../data/train.level.grading')\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1024, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, (test_acc, test_pred, test_gold) = binaryTrainer.val_one_epoch(test_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
