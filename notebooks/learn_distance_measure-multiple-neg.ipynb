{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jiajunb/neural-dimension-reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1d0058850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.models.distance_modeling import SurveyorDataSet, Surveyor, thesis_kl_div_add_mse_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903068df9684e309188e27ba9defcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='create triplets', max=176.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 11520000000 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0898c6cbe620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSurveyorDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jiajunb/neural-dimension-reduction/data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_func2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar_func2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSurveyorDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jiajunb/neural-dimension-reduction/data/dev.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_func2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar_func2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/disk5/neural-dimension-reduction/src/models/distance_modeling.py\u001b[0m in \u001b[0;36mfrom_df\u001b[0;34m(cls, path_to_dataframe, close_fn, far_fn)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                                 \u001b[0mclose_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                                                 close_distance[pairs[:, 1], 0].reshape(-1))\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/disk5/neural-dimension-reduction/src/models/distance_modeling.py\u001b[0m in \u001b[0;36mthesis_input_inverse_similarity\u001b[0;34m(x1, x2, x1_min_dist, x2_min_dist)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mthesis_input_inverse_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_min_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_min_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mdin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdin\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1_min_dist\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSTABLE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdin\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2_min_dist\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSTABLE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 11520000000 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "def far_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, -80:], indices[:, -80:]\n",
    "#     n, d = sorted_dist.shape\n",
    "    \n",
    "#     tmp1, tmp2 = list(), list()\n",
    "#     for i in range(n):\n",
    "#         idx = torch.randint(low=(d - 21), high=(d - 1), size=(1, 20))\n",
    "#         tmp1.append(sorted_dist[i][idx])\n",
    "#         tmp2.append(indices[i][idx])\n",
    "#     return torch.cat(tmp1, dim=0), torch.cat(tmp2, dim=0)\n",
    "\n",
    "def close_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, 1:81], indices[:, 1:81]\n",
    "\n",
    "train_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/train.csv', close_func2, far_func2)\n",
    "val_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/dev.csv', close_func2, far_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of train_dataset: {len(train_dataset)}, size of val_dataset: {len(val_dataset)}')\n",
    "\n",
    "print(f'training set positive: {(train_dataset.labels == 1).sum()} negative: {(train_dataset.labels == 0).sum()}')\n",
    "print(f'validation set positive: {(val_dataset.labels == 1).sum()} negative: {(val_dataset.labels == 0).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-5\n",
    "num_epoches = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "model = Surveyor()\n",
    "\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, verbose):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_sum = 0.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x1, x2, labels, q = batch\n",
    "        x1, x2, labels, q = x1.to(device), x2.to(device), labels.to(device), q.to(device)\n",
    "        logits, p, out1, out2, loss = model(x1, x2, q, labels)\n",
    "        model.zero_grad()  # reset gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        if verbose and i % 20 == 0:\n",
    "            print(f'training loss: {loss_sum / (i + 1):.4f}')\n",
    "    return loss_sum / len(train_loader)\n",
    "\n",
    "def val_one_epoch(val_loader, model):\n",
    "    model.eval()\n",
    "    loss_fn1 = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = thesis_kl_div_add_mse_loss\n",
    "    preds_list = list()\n",
    "    labels_list = list()\n",
    "    val_xentropy_loss = 0.\n",
    "    val_thesis_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x1, x2, labels, q = batch\n",
    "            x1, x2, q = x1.to(device), x2.to(device), q.to(device)\n",
    "            logits, p, out1, out2 = model(x1, x2, q, labels=None)\n",
    "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            preds_list.append(preds.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            labels = labels.to(device)\n",
    "            val_xentropy_loss += loss_fn1(logits, labels).item()\n",
    "            val_thesis_loss += loss_fn2(p, q).item()\n",
    "    y_preds = torch.cat(preds_list)\n",
    "    y_golds = torch.cat(labels_list)\n",
    "    accuracy = float((y_preds == y_golds).sum().item()) / len(y_preds)\n",
    "    return val_xentropy_loss / len(y_preds), val_thesis_loss / len(y_preds), accuracy, y_preds, y_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, verbose):\n",
    "    best_model = None\n",
    "    best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = float('inf'), float('inf'), 0. \n",
    "    for epoch_idx in range(1, num_epoches + 1):\n",
    "        avg_loss = train_one_epoch(train_loader, model, optimizer, False)\n",
    "        avg_xentropy_loss, avg_thesis_loss, val_accuracy, val_y_preds, val_y_golds = val_one_epoch(val_loader, model)\n",
    "        if val_accuracy >  best_val_accuracy:\n",
    "            best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy = avg_xentropy_loss, avg_thesis_loss, val_accuracy\n",
    "            best_model = copy.deepcopy(model.cpu())\n",
    "            print(f'epoch [{epoch_idx}]/[{num_epoches}] training loss: {avg_loss:.4f} '\n",
    "                  f'val_cross_entropy_loss: {avg_xentropy_loss:.4f} '\n",
    "                  f'val_thesis_loss: {avg_thesis_loss:.4f} '\n",
    "                  f'val_accuracy: {val_accuracy:.4f} ')\n",
    "            print(len(val_y_golds))\n",
    "            print(classification_report(val_y_golds, val_y_preds))\n",
    "    return best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1000, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy, best_model, final_model = train_with_eval(train_loader, val_loader, model, optimizer, num_epoches, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_xentropy_loss, best_avg_thesis_loss, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def far_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, 21:], indices[:, 21:]\n",
    "\n",
    "#     n, d = sorted_dist.shape\n",
    "    \n",
    "#     tmp1, tmp2 = list(), list()\n",
    "#     for i in range(n):\n",
    "#         idx = torch.randint(low=(d - 21), high=(d - 1), size=(1, 20))\n",
    "#         tmp1.append(sorted_dist[i][idx])\n",
    "#         tmp2.append(indices[i][idx])\n",
    "#     return torch.cat(tmp1, dim=0), torch.cat(tmp2, dim=0)\n",
    "\n",
    "def close_func2(sorted_dist: torch.tensor, indices: torch.tensor):\n",
    "    return sorted_dist[:, 1:21], indices[:, 1:21]\n",
    "\n",
    "test_dataset = SurveyorDataSet.from_df('/home/jiajunb/neural-dimension-reduction/data/processed/sample/dev.csv', close_func2, far_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1000, pin_memory=True)\n",
    "avg_xentropy_loss, avg_thesis_loss, val_accuracy, val_y_preds, val_y_golds = val_one_epoch(test_loader, best_model.to(device))\n",
    "\n",
    "print(f'val_cross_entropy_loss: {avg_xentropy_loss:.4f} '\n",
    "      f'val_thesis_loss: {avg_thesis_loss:.4f} '\n",
    "      f'val_accuracy: {val_accuracy:.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(val_y_preds == val_y_golds, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 500\n",
    "acc_list = list()\n",
    "for i in range(len(a) // bin_size):\n",
    "    tmp = a[bin_size * i : bin_size * (i + 1)]\n",
    "    acc_list.append(float(tmp.sum().item()) / len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(len(acc_list)), acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     \"best_model\": best_model.state_dict(),\n",
    "#     \"best_avg_xentropy_loss\": best_avg_xentropy_loss,\n",
    "#     \"best_avg_thesis_loss\": best_avg_thesis_loss, \n",
    "#     \"best_val_accuracy\": best_val_accuracy\n",
    "# }, '../saves/surveyor.on.random_select_far.full.100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
